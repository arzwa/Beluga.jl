using TransformVariables, DynamicHMC, LogDensityProblems, Random, CSV,
    DataFrames, ForwardDiff, Beluga, Parameters, Distributions

struct DLWGDProblem
    Î¨::SpeciesTree
    X::PArray
    m::Int64
end

# independent, non-hierarchical prior
function (problem::DLWGDProblem)(Î¸)
    @unpack X, Î¨, m = problem
    @unpack Î», Î¼, q, Î· = Î¸
    # priors
    ll = logpdf(Beta(8,2), Î·)
    ll += sum(logpdf.(Beta(), q))
    ll += sum(logpdf.(Exponential(), Î»))
    ll += sum(logpdf.(Exponential(), Î¼))
    # lhood
    d = DuplicationLossWGD(Î¨, Î», Î¼, q, Î·, m)
    ll += logpdf(d, X)
    ll
end

# data
s = SpeciesTree("test/data/tree1.nw")
df = CSV.read("test/data/counts1.tsv", delim="\t")
deletecols!(df, :Orthogroup)
p, m = Profile(df, s)
n = Beluga.nrates(s)

# problem and inits
problem = DLWGDProblem(s, p, m)
Î¸ = (Î»=exp.(randn(n)), Î¼=exp.(randn(n)), q=rand(Beluga.nwgd(s)), Î·=0.8)
@show problem(Î¸)

# transformation
problem_transformation(p::DLWGDProblem) =
    as((Î»=as(Array, asâ„â‚Š, n),
        Î¼=as(Array, asâ„â‚Š, n),
        q=as(Array, asð•€, nwgd(p.Î¨)),
        Î·=asð•€))
P = TransformedLogDensity(problem_transformation(problem), problem)
âˆ‡P = ADgradient(:ForwardDiff, P)

# mcmc
results = mcmc_with_warmup(Random.GLOBAL_RNG, âˆ‡P, 10)


d = DuplicationLoss(s, exp.(randn(n)), exp.(randn(n)), 0.8, m)
gradient(d, p) |> show
